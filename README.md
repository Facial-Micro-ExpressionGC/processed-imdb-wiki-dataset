# Processed IMDB WIKI Dataset

This GitHub repository contains complete preprocessed IMDB WIKI dataset in `.csv` files.

<p align="center">
  <img src="https://user-images.githubusercontent.com/34741145/51108233-75bac680-1817-11e9-8b79-6a1ee05d8aa4.png" />
</p>

## Table of contents:
- [Introduction](#introduction)
- [IMDB WIKI Dataset](#imdb-wiki-dataset)
- [The Problem](#the-problem)
- [The Solution](#the-solution)
- [File Structure](#file-structure)
- [How to Run Locally](#how-to-run-locally)
- [Acknowledgments](#acknowledgments)

## Introduction
IMDB WIKI dataset is the largest dataset of human faces with gender, name and age informations. In the dataset, images are of `.jpg` images and meta informations are in `.mat` files. 

In this project, I preprocessed all images, resized them, extract the meta informations from the `.mat` files and finally save them in `.csv` files into multiple batches.


## IMDB WIKI Dataset
IMDB WIKI dataset is the largest publically available dataset of human faces with gender, age, and name. It contains more than `500 thousand+` images with all the meta informations. 

For more information about the dataset please visit [this website](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/).

## The Problem
problem

## The Solution
solution

## File Structure
files

## How to Run Locally
locally

## Acknowledgments
Acknowledgments
